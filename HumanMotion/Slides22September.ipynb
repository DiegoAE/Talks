{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Human-motion Synthesis through Physically-inspired Machine Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Diego A. Agudelo España.\n",
    "\n",
    "#### Tutor: Mauricio A. Álvarez, Phd."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## General Aim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* To develop a general methodology for computer-generation of human motion figures based on\n",
    "mechanistically inspired machine learning methods. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Current Specific Aim **:\n",
    "*  To develop a non-parametric sequential dynamical model for describing the time evolution of motor primitives in motion capture data and humanoid robotics performing different tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Our methodologies will use general non-linear\n",
    "regression methods that incorporate flexible and soft mechanistic assumptions for combining models\n",
    "of basic movements either hierarchically or sequentially.\n",
    "\n",
    "* **Data driven approach (weakly mechanistic)**:\n",
    "    * if data is scarce in comparison with the model it could be unable to make accurate predictions.\n",
    "* ** Purely mechanistic models (strongly mechanistic)**:\n",
    "    * A mechanistic model can enable accurate predictions even in regions where there is no available training data but the model could be extremely complex.\n",
    "* **  Proposed Approach (hybrid system) **:\n",
    "    * Based on the observation that a weak mechanistic assumption underlie a data driven model.\n",
    "    * The key is to retain sufficient flexibility in our model to be able to fit the system even when the mechanistic assumptions are not rigorously fulfilled.\n",
    "    \n",
    "What is motor primitive? is the basic building block complex movements are made up of."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Proposed Approach "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Motor primitive representation**: Latent Force Models (LFM's).\n",
    "* **Sequential dynamical model**: Hidden Markov Models (HMM's)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Mention the reason why a single LFM is not suitable for the problem of interest (discontinuities over latent forces and discrete changes, GP smoothnes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Second Order Latent Force Models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{d^2 y_d(t)}{dt} + C_d \\frac{d y_d(t)}{dt} + B_d y(t) = \\sum_{q=1}^Q S_{d,q} u_q(t)$$\n",
    "\n",
    "If Gaussian process priors are assumed over the latent forcing functions with RBF covariance functions then the output functions are jointly governed by a Gaussian process as well. Formally,\n",
    "\n",
    "$$Y \\sim GP(0, K(x, x'; \\Theta))$$\n",
    "\n",
    "Where $Y$ represents the stacked system of output functions, $K$ represents the second order latent force model kernel (Alvarez et al, 2009) and $\\Theta$ represents the set of kernel hyperparameters (i.e. differential equation coefficients and lengthscales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "* Gaussian Process priors with RBF covariance are assumed over the latent forcing functions.\n",
    "* In this models the system is being forced by latent functions (LFM).\n",
    "* The general framework in LFM is to combine a mechanistic model with a probabilistic prior over a latent variable/function.\n",
    "*  allows combining dimensionality reduction with systems of differential equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## HMM + LFM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The whole system is modelled as a Hidden Markov Model (**HMM**) where each hidden state represents a different motor primitive. The emission distribution for each hidden state is represented by a Latent Force Model (**LFM**). The corresponding graphical model is the following:\n",
    "\n",
    "<img src=\"http://d29qn7q9z0j1p6.cloudfront.net/content/roypta/371/1984/20120222/F4.large.jpg?width=800&height=600&carousel=1\" >\n",
    "\n",
    "$$\\text{Figure from (Bishop, 2013).}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## HMM + LFM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now the following assumptions are made:\n",
    "\n",
    "* There is only one latent force governing the movements.\n",
    "* The number of sample locations belonging to each LFM is the same and is denoted by $N$. All of them are assumed to be equally spaced.\n",
    "* The number of segments is fixed and is equal to $W$.\n",
    "* Each segment (i.e hidden state, LFM) can potentially have a different set of parameters (e.g. damper constants, spring constants, etc.)\n",
    "* The hidden state $z_n$ only depends on the state $z_{n-1}$ and this dependency is represented via the transition probability distribution $A_{z_{n-1}}$.\n",
    "* The emission $Y_i = \\{y^i_1, y^i_2, \\dots, y^i_N\\}$ for the sample locations $X_i = \\{x^i_1, x^i_2, \\dots, x^i_N\\}$ only depends on the current hidden state $z_i$.\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## HMM + LFM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formally,\n",
    "\n",
    "The joint probability of $Y = \\{ Y_1, Y_2, \\dots, Y_W \\}$ and $Z = \\{ z_1, z_2, \\dots, z_W \\}$ given $X = \\{ X_1, X_2, \\dots, X_W \\}$ and $\\Theta = \\{ \\theta_1, \\theta_2, \\dots, \\theta_W \\}$.\n",
    "\n",
    "$$p(Z, Y | X, \\Theta) = p(z_1 | s) p(Y_1 | z_1, \\theta_1, X_1) \\prod_{i = 2}^{W} p(z_i | z_{i - 1}, A) p(Y_i | z_i, \\theta_i, X_i)$$\n",
    "\n",
    "where $A$ is the hidden state transition probabiliy matrix, $s$ is initial state probability distribution and $\\theta_i$ is the set of parameters which characterize the i-th LFM.\n",
    "\n",
    "The emission process is performed as follows:\n",
    "\n",
    "$$p(Y_i | z_i, \\theta_i, X_i) = \\mathcal{N}(f_i(X_i), I \\sigma^2)$$\n",
    "\n",
    "where:\n",
    "\n",
    "$$f_i(x) \\sim \\mathcal{GP}(0, k_f(x, x'; \\theta_i))$$.\n",
    "\n",
    "and $k_f$ represents the second order LFM kernel.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How to infer the model from data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maximization of the expected marginal log likelihood over $\\Theta$ (this implies taking kernel derivatives w.r.t. hyperparameters). \n",
    "\n",
    "$$ \\ln{p(Z, Y | X, \\Theta)} = \\sum_{k=1}^K z_{1,k} \\ln{s_k} + \\sum_{n=2}^W \\sum_{k=1}^K \\sum_{j=1}^K  z_{n, i} z_{n - 1, j} \\ln{A_{j,k}} + \\sum_{n=1}^W \\sum_{k=1}^K z_{n,k} \\ln{p(Y_n |\\theta_k, X_n)} $$\n",
    "\n",
    "The maximization w.r.t parameters $s_k$ and $A_{j,k}$ is performed using the standard EM algorithm. Notice that parameters are not coupled in the optimization process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Maximization step for emission parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the aim is to optimize the following term w.r.t. the hyperparameters $\\theta_k$:\n",
    "\n",
    "$$ E = \\sum_{k=1}^K  \\sum_{n=1}^W \\gamma(z_{n,k}) \\ln{p(Y_n |\\theta_k, X_n)} $$\n",
    "\n",
    "where:\n",
    "\n",
    "$$\\gamma(z_{i,j}) = \\langle z_{i,j} \\rangle_{p(Z | X, \\Theta^{old})} = p(z_{i,j} = 1 | X, \\Theta^{old} ) = p(z_i = j | X, \\Theta^{old} )$$\n",
    "\n",
    "In order to use gradient ascent is necessary to take derivatives w.r.t the kernel hyperparameters. Assuming $\\theta_k = \\{\\theta_{k,1}, \\dots, \\theta_{k,h} \\}$ we have (Rassmussen, 2006) <cite data-cite=\"rasmussen2006gaussian\"></cite>:\n",
    "\n",
    "$$ \\frac{\\partial E}{\\partial \\theta_{k,j}} =  \\sum_{n=1}^W \\gamma(z_{n,k}) \\frac{\\partial \\ln p(Y_n | \\theta_k, X_n)}{\\partial \\theta_{k,j}}$$\n",
    "\n",
    "$$ \\text{with}$$\n",
    "\n",
    "$$ \\frac{\\partial \\ln p(Y_n | \\Theta_k, X_n)}{\\partial \\theta_{k,j}} = \\frac{1}{2} tr \\left( (\\alpha \\alpha^T - K_y^{-1}) \\frac{\\partial K_y}{\\partial \\theta_{k,j}} \\right) \\text{ with } \\alpha = K_y^{-1} Y_n$$\n",
    "\n",
    "It is clear that the result of $\\frac{\\partial K_y}{\\partial \\theta_{k,j}}$ is an elementwise derivative and depends on the specific form of the kernel $k_f$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Currently working in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Inferring the standard Latent Force Model (LFM) parameters from data.\n",
    "* Plugging the LFM inference over the standard EM algorithm for HMM."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
