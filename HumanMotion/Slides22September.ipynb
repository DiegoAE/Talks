{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Human-motion Synthesis through Physically-inspired Machine Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## General Aim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* To develop a general methodology for computer-generation of human motion figures based on\n",
    "mechanistically inspired machine learning methods. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Current Specific Aim **:\n",
    "*  To develop a non-parametric sequential dynamical model for describing the time evolution of motor primitives in motion capture data and humanoid robotics performing different tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Our methodologies will use general non-linear\n",
    "regression methods that incorporate flexible and soft mechanistic assumptions for combining models\n",
    "of basic movements either hierarchically or sequentially.\n",
    "\n",
    "* **Data driven approach (weakly mechanistic)**:\n",
    "    * if data is scarce in comparison with the model it could be unable to make accurate predictions.\n",
    "* ** Purely mechanistic models (strongly mechanistic)**:\n",
    "    * A mechanistic model can enable accurate predictions even in regions where there is no available training data but the model could be extremely complex.\n",
    "* **  Proposed Approach (hybrid system) **:\n",
    "    * Based on the observation that a weak mechanistic assumption underlie a data driven model.\n",
    "    * The key is to retain sufficient flexibility in our model to be able to fit the system even when the mechanistic assumptions are not rigorously fulfilled.\n",
    "    \n",
    "What is motor primitive? is the basic building block complex movements are made up of."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Proposed Approach "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Motor primitive representation**: Latent Force Models (LFM's).\n",
    "* **Sequential dynamical model**: Hidden Markov Models (HMM's)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Mention the reason why a single LFM is not suitable for the problem of interest (discontinuities over latent forces and discrete changes, GP smoothnes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Second Order Latent Force Models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{d^2 y_d(t)}{dt} + C_d \\frac{d y_d(t)}{dt} + B_d y(t) = \\sum_{q=1}^Q S_{d,q} u_q(t)$$\n",
    "\n",
    "If Gaussian process priors are assumed over the latent forcing functions with RBF covariance functions then the output functions are jointly governed by a Gaussian process as well. Formally,\n",
    "\n",
    "$$Y \\sim GP(0, K(x, x'; \\Theta))$$\n",
    "\n",
    "Where $Y$ represents the stacked system of output functions, $K$ represents the second order latent force model kernel (Alvarez et al, 2009) and $\\Theta$ represents the set of kernel hyperparameters (i.e. differential equation coefficients and lengthscales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "* Gaussian Process priors with RBF covariance are assumed over the latent forcing functions.\n",
    "* In this models the system is being forced by latent functions (LFM).\n",
    "* The general framework in LFM is to combine a mechanistic model with a probabilistic prior over a latent variable/function.\n",
    "*  allows combining dimensionality reduction with systems of differential equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## HMM + LFM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The whole system is modelled as a Hidden Markov Model (**HMM**) where each hidden state represents a different motor primitive. The emission distribution for each hidden state is represented by a Latent Force Model (**LFM**). The corresponding graphical model is the following:\n",
    "\n",
    "<img src=\"http://d29qn7q9z0j1p6.cloudfront.net/content/roypta/371/1984/20120222/F4.large.jpg?width=800&height=600&carousel=1\" >\n",
    "\n",
    "$$\\text{Figure from (Bishop, 2013).}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## HMM + LFM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now the following assumptions are made:\n",
    "\n",
    "* There is only one latent force governing the movements.\n",
    "* The number of sample locations belonging to each LFM is the same and is denoted by $N$. All of them are assumed to be equally spaced.\n",
    "* The number of segments is fixed and is equal to $W$.\n",
    "* Each segment (i.e hidden state, LFM) can potentially have a different set of parameters (e.g. damper constants, spring constants, etc.)\n",
    "* The hidden state $\\zeta_i$ only depends on the state $\\zeta_{i-1}$ and this dependency is represented via the transition probability distribution $\\pi_{\\zeta_{i-1}}$.\n",
    "* The emission $Y_i = \\{y^i_1, y^i_2, \\dots, y^i_N\\}$ for the sample locations $X_i = \\{x^i_1, x^i_2, \\dots, x^i_N\\}$ only depends on the current hidden state $\\zeta_i$.\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## HMM + LFM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formally,\n",
    "\n",
    "The joint probability of $Y = \\{ Y_1, Y_2, \\dots, Y_W \\}$ and $\\Gamma = \\{ \\zeta_1, \\zeta_2, \\dots, \\zeta_W \\}$ given $X = \\{ X_1, X_2, \\dots, X_W \\}$\n",
    "\n",
    "$$p(\\Gamma, Y | X) = p(\\zeta_1 | s) p(Y_1 | \\zeta_1, \\theta_1) \\prod_{i = 2}^{W} p(\\zeta_i | \\zeta_{i - 1}, \\pi) p(Y_i | \\zeta_i, \\theta_i, X_i)$$\n",
    "\n",
    "where $\\pi$ is the hidden state transition probabiliy matrix, $s$ is initial state probability distribution and $\\theta_i$ is the set of parameters which characterize the i-th LFM.\n",
    "\n",
    "The emission process is performed as follows:\n",
    "\n",
    "$$p(Y_i | \\zeta_i, \\theta_i, X_i) = \\mathcal{N}(f_i(X_i), I \\sigma^2)$$\n",
    "\n",
    "where:\n",
    "\n",
    "$$f_i(x) \\sim \\mathcal{GP}(0, K(x, x'; \\theta_i))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How to infer the model from data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A particular approach found in literature:\n",
    "   * Variational EM Method.\n",
    "   * Maximization of the expected marginal log likelihood over $\\Theta$ (this implies taking kernel derivatives w.r.t. hyperparameters). "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
